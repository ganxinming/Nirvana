# CAP

- Consistency 一致性
- Availability 可用性
- Partition tolerance 分区容错性

CAP其实是一种权衡平衡的思想，用于指导在系统可用性设计、数据一致性设计时做权衡取舍。

分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。

(分布式系统一定会满足其中两个特征)



### P 分区容错性

每个网络区域，可以认为是一个区，而每个区进行通信，我们需要保证他们能够正常进行通信，这是基础。所以这个特性必有

分布式系统在遇到网络故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障。

(网络问题必定存在，所以P必定需要，除非单台服务器，但是那就不是分布式了)



### C一致性

数据可能存在于不同的物理节点上,节点之间只能通过网络进行通信来协调彼此之间的状态,而网络通信需要时间并且其本身并不十分可靠,因而如何保持数据一致性成为了分布式系统的难题。

分布式系统有多台机器，每一次的更新，插入操作，后面对应的必定是有多台机器，插入到A中，确保B也会插入。

因为访问数据时，请求可能访问A，也可能是B，为了保证结果一致，所以在更新A时，B也必须更新。



#### A可用性

很简单，发起一次操作后，不管是A也好，B也好，总是你必须及时回应请求。



因为P是必定有的，那为什么C和A是排斥的呢，如果想要保持C，那么两台机器之间进行数据同步，可能出现延时或同步失败的情况，这段时间，机器就是不可用的，那么就不满足可用性了。



（是不是觉得P和A很像，其实两者服务对象不一样，P指的是内部系统之间调用出现故障，也能保持内部各服务之间稳定调用。 A指的是系统挂了，其他的系统能继续为用户提供服务）



# 幂等性

**所谓幂等，就是任意多次执行所产生的影响均与一次执行的影响相同**

(多次操作和一次操作产生的影响相同，如查询接口，天然的幂等性，只是查询没有其他影响)





## 1.zookeeper：分布式一致性服务=文件系统+监听通知机制

ZooKeeper是一种为分布式应用所设计的高可用、高性能且一致的开源协调服务，它提供了一项基本服务：**分布式锁服务**。由于ZooKeeper的开源特性，后来我们的开发者在分布式锁的基础上，摸索了出了其他的使用方法：**配置维护、组服务、分布式消息队列**、**分布式通知/协调**等。

### 1.1功能:统一命名服务、状态同步服务、配置维护、集群管理。(都是基于分布式锁)

（因为分布式系统，对于某些服务器地址的管理，比如：mysql，dubbo的地址。如果机器的地址要变动呢？数量一多，那不就得一个个去改吗？很麻烦。此时只要将他们交给zookeeper管理，我们只要拿到zookeeper地址，就可以管理这些所有地址。）

原理：在目录下创建这些文件，然后存入这些节点。监听这些节点的变化，然后通知给客户端。

首先它设计一种新的**数据结构——Znode**，然后在该数据结构的基础上定义了一些**原语**，也就是一些关于该数据结构的一些操作。有了这些数据结构和原语还不够，因为我们的ZooKeeper是工作在一个分布式的环境下，我们的服务是通过消息以网络的形式发送给我们的分布式应用程序，所以还需要一个**通知机制**——Watcher机制。那么总结一下，ZooKeeper所提供的服务主要是通过：数据结构+原语+watcher机制，三个部分来实现的。

Watcher：客户端可以在节点上设置Watcher（可以叫做监视器）。当节点状态发生变化时，就会触发监视器对应的操作，当监视器被触发时，ZK服务器会向客户端发送且只发送一个通知

数据访问：Zookeeper上存储的数据需要被原子性的操作（要么修改成功要么回到原样），也是就读操作将会读取节点相关所有数据，写操作也会修改节点相关所有数据，，而且每个节点都有自己的ACL。
有四种类型的znode：

节点类型：Zookeeper中有几种节点类型，节点类型在节点创建的时候就被确定且不可改变

- **PERSISTENT-持久化目录节点**

  客户端与zookeeper断开连接后，该节点依旧存在，创建后永久存在，除非主动删除。(默认)

- **PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点**

  客户端与zookeeper断开连接后，该节点依旧存在，所谓顺序节点，就是在创建节点时，Zookeeper根据创建的时间顺序给该节点名称进行编号。例如先创建了节点/xw_persistent_sequential0000000001，则第二个节点/xw_persistent_sequential0000000002

  该节点创建后持久存在，相对于持久节点它会在节点名称后面自动增加一个10位数字的序列号，这个计数对于此节点的父节点是唯一，如果这个序列号大于2^32-1就会溢出。

- **EPHEMERAL-临时目录节点**

  客户端与zookeeper断开连接后，该节点被删除。临时创建的，会话结束节点自动被删除，也可以手动删除，==临时节点不能拥有子节点。==

- **EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点**（解决分布式锁(在分布式情况下，线程不在同一个机器上，锁失效了)的问题：保证来的第一个拿到锁，导致后面的节点不断监听前面的节点，形成一条链。当然redis也可以解决分布式Setnx()方法保证独占）

  在创建节点时，Zookeeper根据创建的时间顺序给该节点名称进行编号；当创建节点的客户端与zookeeper断开连接后，临时节点会被删除。

## 2.Simple API

One of the design goals of ZooKeeper is providing a very simple programming interface. As a result, it supports only these operations:

- *create* : creates a node at a location in the tree
- *delete* : deletes a node
- *exists* : tests if a node exists at a location
- *get data* : reads the data from a node
- *set data* : writes data to a node
- *get children* : retrieves a list of children of a node
- *sync* : waits for data to be propagated





## Znode的状态属性

cZxid	创建节点时的事务ID
ctime	创建节点时的时间
mZxid	最后修改节点时的事务ID
mtime	最后修改节点时的时间
pZxid	表示该节点的子节点列表最后一次修改的事务ID，添加子节点或删除子节点就会影响子节点列表，但是修改子节点的数据内容则不影响该ID（注意，只有子节点列表变更了才会变更pzxid，子节点内容变更不会影响pzxid）
cversion	子节点版本号，子节点每次修改版本号加1
dataversion	数据版本号，数据每次修改该版本号加1
aclversion	权限版本号，权限每次修改该版本号加1
ephemeralOwner	创建该临时节点的会话的sessionID。（如果该节点是持久节点，那么这个属性值为0）
dataLength	该节点的数据长度
numChildren	该节点拥有子节点的数量（只统计直接子节点的数量）



## Watcher特点

主动推送：触发watcher时，zk主动推送到客户端

一次性：数据变化时，watcher只会触发一次

 可见性：先通知，

# 常用命令：

./zkServer start 启动

./zkServer status 查看状态，有时候启动了不一定起来了

./zkServer stop  关闭

./ 进入客户端



create /test 创建节点(test)

ls / 显示根目录下节点 。 Ls /test 目录下节点

create -s {path}  创建有序节点（相同名字会后面添加十位递增的数字后缀。）

create -e {path}  创建临时节点  当断开连接后，临时节点会被删除。

set /tmp/test10000000000 123 （设置值）
get /tmp/test10000000000  (获得值)

delete /tmp/test10000000000   （删除节点）



# zk集群

主节点leader：负责整个集群写操作和读操作，保证事务处理的顺序，以及主从，observer节点的数据同步

从节点flower：负责读请求，不处理写请求，而是转发到leader节点进行写入，参与leader选举

观察节点observer：特殊节点，不参与leader选举，其他和flower功能一样，仅仅是为了提升读操作 吞吐量





## 写操作流程

通过Leader进行写操作，主要分为五步：

1. 客户端向Leader发起写请求，接收到写请求，会赋予一个全局唯一的zxid，通过zxid保证写操作的顺序一致性。（64位自增id）
2. Leader将写请求以==Proposal(提案)==的形式发给所有Follower并等待ACK
3. Follower收到Leader的Proposal后，将proposal写入到本地事务日志，并返回ACK
4. Leader得到过半数的ACK（Leader对自己默认有一个ACK）后向所有的Follower和Observer发送==Commmit==，并且本地执行提交
5. flower收到commit后，也会提交操作，并返回client写请求响应(返回客户端调用)

<img src="../../../Library/Application Support/typora-user-images/image-20220428220719938.png" alt="image-20220428220719938" style="zoom:33%;" />

## Leader节点选举

leader节点在什么时候会被选举呢？主要有以下两个时机：

* 服务器初始化启动（当集群刚启动时，这时集群中没有leader节点，需要选取出一个leader节点）

* 集群运行期间Leader节点异常（leader节点服务异常，其他节点无法和leader节点保持通信，那么就会重新选择一个leader）

1.==Zookeeper集群中服务器被划分为以下四种状态==

- LOOKING：寻找Leader状态。处于该状态的服务器会认为集群中没有Leader，需要进行Leader选举；
- FOLLOWING：跟随着状态，说明当前服务器角色为Follower；
- LEADING：领导者状态，表明当前服务器角色为Leader；
- OBSERVING：观察者状态，表明当前服务器角色为Observer。

2.选票信息

==每个选票中包含四个最基本的信息，服务器的ID，数据ID，逻辑时钟以及选举状态：==

- 服务器ID：即myId，服务器的唯一标识，编号越大在选择算法中的权重越大；(唯一标识)
- 数据ID：服务器中存放的最大数据ID，值越大说明数据越新，在选举算法中数据新权重越大；（ZXID 64位自增id）
- 逻辑时钟：Epoch，逻辑时钟，或者叫投票次数，同一轮投票过程时钟值是相同的，每投完一次票这个数据就会增加，然后与接收到的其他服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断；
- 选举状态：即上文提到的四种状态。



ZXID = epoch(纪元 高32位) 和自增计数器 (低 32位)

### 3.服务器启动时期的Leader选举

假设想在的有三台机器搭建集群，在集群初始化阶段，当只有一个服务器（Server1）启动时，无法完成Leader的选举；当第二台服务器（Server2）启动后，两台机器开始互相通信，每台机器都会尝试去选举Leader，于是进入了Leader选举过程，这个过程大概如下：

1. **每个Server发出一个投票投给自己**。在初始情况下，Server1和Server2都会将自己作为Leader，将票投给自己。每次投票会包含所推举的服务器的myid和ZXID，使用（myid，ZXID）来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中的其他机器；

2. 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票是否有效，如检查是否是本轮投票、是否来自LOOKING状态的服务器；

3. 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK的规则如下：

   对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，==首先会比较两者的ZXID，均为0，再比较myid==，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。(更新投票)

4. 1. 优先检查ZXID。ZXID比较大的服务器优先作为Leader；(ZXID还是比较公平的，全靠自己去抢占生成)
   2. 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。(myid是确定的，跟每台机器有关)

5. **统计投票**。每次投票后，服务器都会统计投票信息，判断是否已经过半机器接收到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。

6. **改变服务器状态**。一旦确定了Leader，每个服务器都会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。

7. ==leader节点会将epoch值+1==，并将新生成的epoch分发给各个follower节点，各个节点收到新的epoch返回ACK，并带上各自最大的zxid。

   

### 4.服务器运行期间的 Leader 选举

在Zookeeper运行期间，==即便有新服务器加入，也不会影响到Leader==，新加入的服务器会将原有的Leader服务器视为Leader，进行同步。但是一旦Leader宕机了，那么整个集群就将暂停对外服务，进行新一轮Leader的选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。选举过程如下：

1. ==变更LOOKING状态==。Leader宕机后，余下的非Observer服务器都会将自己的服务器状态变更为LOOKING，然后开始进行Leader选举流程；
2. **每个Server会发出一个投票**。在这个过程中，需要生成投票信息(myid,ZXID)每个服务器上的ZXID可能不同，我们假定Server1的ZXID为123，而Server3的ZXID为122；在第一轮投票中，Server1和Server3都会投自己，产生投票(1, 123)，(3, 122)，然后各自将投票发送给集群中所有机器；
3. 接收来自各个服务器的投票。与启动时过程相同；
4. 处理投票；
5. 统计投票；
6. 改变服务器的状态。

(和启动选举差不多，只不过将状态全部置为LOOKING，其他一致)
